{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/malinphy/LangChain/blob/main/LangChain_intro.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lj6cbR0JBTCP"
      },
      "source": [
        "!pip install openai -q\n",
        "!pip install langchain -q\n",
        "!pip install google-search-results -q\n",
        "import os \n",
        "os.environ[\"OPENAI_API_KEY\"] = \n",
        "os.environ['SERPAPI_API_KEY'] = "
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import OpenAI\n",
        "from langchain.prompts import PromptTemplate"
      ],
      "metadata": {
        "id": "UweeQOY_BeGQ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = OpenAI(temperature = 0.9)"
      ],
      "metadata": {
        "id": "nWDCgDzqBgFO"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = 'What are 5 vacation destinations for someone who like to eat pasta'\n",
        "print(llm(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CK8aZtCeBht3",
        "outputId": "0511510f-15bf-4498-dda0-ea8b51275ccf"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "1. Rome, Italy \n",
            "2. Venice, Italy \n",
            "3. Florence, Italy \n",
            "4. Tuscany, Italy \n",
            "5. Amalfi Coast, Italy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = PromptTemplate(\n",
        "    input_variables = ['food'],\n",
        "    template = 'what are 5 vacation destinations for someone who like to eat {food}'\n",
        "    )"
      ],
      "metadata": {
        "id": "KQ6tZPcFBjHa"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(prompt.format(food = 'dessert'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sMe7RXHdBlbW",
        "outputId": "f11f6728-8c49-47e8-c283-7280aee0fd7f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "what are 5 vacation destinations for someone who like to eat dessert\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(llm(prompt.format(food= 'kebab')))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VKX7-jcWBneH",
        "outputId": "0501bccf-986b-4f83-b6f5-457cae7ddae5"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "1. Turkey \n",
            "2. Iran \n",
            "3. Greece\n",
            "4. Egypt \n",
            "5. Morocco\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.chains import LLMChain"
      ],
      "metadata": {
        "id": "z5M12TgABpOY"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = OpenAI(temperature = 0.9)\n",
        "prompt = PromptTemplate(\n",
        "    input_variables = ['food'],\n",
        "    template = 'What are 5 vacation destinations for someone who likes to eat {food}'\n",
        "    )"
      ],
      "metadata": {
        "id": "0gc0lGo9BqlI"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain = LLMChain(llm = llm, prompt = prompt)"
      ],
      "metadata": {
        "id": "-Uta4QSSBsEP"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(chain.run('fruit'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "drwZw8iwBtdP",
        "outputId": "f24b7408-93f5-4982-9312-dd95d2bda7a4"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "1. Bali, Indonesia - Enjoy lush tropical fruits like durian, mangosteen, and dragonfruit\n",
            "2. Guadalajara, Mexico - Sample delicious fruits like guava, mango, and dragonfruit\n",
            "3. Hanoi, Vietnam - Delight in exotic exotic fruits like rambutan, longan and jackfruit\n",
            "4. Seychelles - Indulge in the unique fruits like coco de mer palm and Seychelles nutmeg\n",
            "5. Rio De Janeiro, Brazil - Feast on weird and wonderful fruits like acai, cupuaÃ§u, and bacuri.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents import load_tools\n",
        "from langchain.agents import initialize_agent\n",
        "from langchain.llms import OpenAI"
      ],
      "metadata": {
        "id": "tbpcIyNkBvSP"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = OpenAI(temperature = 0.9)\n",
        "tools = load_tools(['serpapi', 'llm-math'], llm = llm)"
      ],
      "metadata": {
        "id": "wXaEJcbLBw4X"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent = initialize_agent(tools, llm, agent = 'zero-shot-react-description', verbose = True)"
      ],
      "metadata": {
        "id": "Co-bSoyECGhg"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent.run('who is the current leader of Japan? What is the largest prime that is smaller than their age?')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 413
        },
        "id": "wmkeEORwC2vp",
        "outputId": "9fdf13d5-94dc-4f04-d05d-0e8d7050ef50"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m I need to find out the leader of Japan and their age.\n",
            "Action: Search\n",
            "Action Input: current leader of Japan\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3mFumio Kishida\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m Now I have the leader of Japan, I need to know their age.\n",
            "Action: Search\n",
            "Action Input: age of Fumio Kishida\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3m65 years\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m Now I know the leader of Japan and their age, I can find the largest prime that is smaller than their age.\n",
            "Action: Calculator\n",
            "Action Input: 65\u001b[0m\n",
            "Observation: \u001b[33;1m\u001b[1;3mAnswer: 65\n",
            "\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer\n",
            "Final Answer: The largest prime that is smaller than the age of the current leader of Japan is 61.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The largest prime that is smaller than the age of the current leader of Japan is 61.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Memory: Add State to chains and agents"
      ],
      "metadata": {
        "id": "O35nGx3eDoA_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import OpenAI, ConversationChain"
      ],
      "metadata": {
        "id": "OdQPjEHODgNS"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = OpenAI(temperature = 0.9)\n",
        "conversation = ConversationChain(llm = llm, verbose = True)"
      ],
      "metadata": {
        "id": "gFaa-PGmFdR5"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.predict(input = 'Hi there')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 270
        },
        "id": "A4SWVeu2FtYH",
        "outputId": "3d3c089d-4f9a-4bb6-8da7-6a9d948f4cc4"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "\n",
            "Human: Hi there\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' Hi! How are you today?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.predict(input = 'The weather is terrible today and I am feeling terrible.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "id": "IZJbcbtbFy5w",
        "outputId": "0a1632c2-8b0d-4012-85db-3edc70376cc6"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: Hi there\n",
            "AI:  Hi! How are you today?\n",
            "Human: The weather is terrible today and I am feeling terrible.\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' Oh no! That sounds really awful. Is there anything I can do to help you feel better?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.predict(input = 'I hope, you can give me advice to feel better.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341
        },
        "id": "u1SvM_o3F6xK",
        "outputId": "a266e85f-970c-4497-d803-6efcb21a9d4d"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: Hi there\n",
            "AI:  Hi! How are you today?\n",
            "Human: The weather is terrible today and I am feeling terrible.\n",
            "AI:  Oh no! That sounds really awful. Is there anything I can do to help you feel better?\n",
            "Human: I hope, you can give me advice to feel better.\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\" I'm sorry, I don't have any advice. But I can provide moral support if you'd like. Is there anything else I can do for you?\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9rJ_7PznHKTq"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EfzwoiElHQIq"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ulCYoiHRHUWC"
      },
      "execution_count": 20,
      "outputs": []
    }
  ]
}