{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/malinphy/LangChain/blob/main/LangChain_intro.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "LynUObK34ZTx",
        "outputId": "c72c0f72-44e1-4e3d-e0fb-f46e37cc7dc5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lj6cbR0JBTCP",
        "outputId": "e2ce2ea1-42d8-4562-bda8-9e63695cfb2b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install openai -q\n",
        "!pip install langchain -q\n",
        "!pip install google-search-results -q\n",
        "import os \n",
        "os.environ[\"OPENAI_API_KEY\"] =  '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' \n",
        "os.environ['SERPAPI_API_KEY'] =  '06308305345bc9d73b8207f48b29d3f871e7a6c28e886739d62591f264da5366' "
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.1/70.1 KB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m264.6/264.6 KB\u001b[0m \u001b[31m33.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.8/158.8 KB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.2/114.2 KB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m420.2/420.2 KB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.1/49.1 KB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for google-search-results (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import OpenAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "UweeQOY_BeGQ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = OpenAI(temperature = 0.9)"
      ],
      "metadata": {
        "id": "nWDCgDzqBgFO"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = 'What are 5 vacation destinations for someone who like to eat pasta'\n",
        "print(llm(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CK8aZtCeBht3",
        "outputId": "9acce46d-dd92-4c60-a82f-110a0c9545cc"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "1. Rome, Italy \n",
            "2. Venice, Italy \n",
            "3. Amalfi Coast, Italy \n",
            "4. Sicily, Italy \n",
            "5. Bologna, Italy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = PromptTemplate(\n",
        "    input_variables = ['food'],\n",
        "    template = 'what are 5 vacation destinations for someone who like to eat {food}'\n",
        "    )"
      ],
      "metadata": {
        "id": "KQ6tZPcFBjHa"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(prompt.format(food = 'dessert'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sMe7RXHdBlbW",
        "outputId": "fadd9c29-d216-493c-fc3f-7febc5eaf11c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "what are 5 vacation destinations for someone who like to eat dessert\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(llm(prompt.format(food= 'kebab')))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VKX7-jcWBneH",
        "outputId": "0973118d-f67e-4b66-ad27-67890f02714d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "1. Istanbul, Turkey\n",
            "2. Aleppo, Syria \n",
            "3. Amman, Jordan\n",
            "4. Tehran, Iran\n",
            "5. Delhi, India\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.chains import LLMChain"
      ],
      "metadata": {
        "id": "z5M12TgABpOY"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = OpenAI(temperature = 0.9)\n",
        "prompt = PromptTemplate(\n",
        "    input_variables = ['food'],\n",
        "    template = 'What are 5 vacation destinations for someone who likes to eat {food}'\n",
        "    )"
      ],
      "metadata": {
        "id": "0gc0lGo9BqlI"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain = LLMChain(llm = llm, prompt = prompt)"
      ],
      "metadata": {
        "id": "-Uta4QSSBsEP"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(chain.run('fruit'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "drwZw8iwBtdP",
        "outputId": "9ae3ab3f-2ee8-4a73-da5a-2aee113c85e6"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "1. Hawaii \n",
            "2. Dominican Republic \n",
            "3. Costa Rica\n",
            "4. Thailand\n",
            "5. Jamaica\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents import load_tools\n",
        "from langchain.agents import initialize_agent\n",
        "from langchain.llms import OpenAI"
      ],
      "metadata": {
        "id": "tbpcIyNkBvSP"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = OpenAI(temperature = 0.9)\n",
        "tools = load_tools(['serpapi', 'llm-math'], llm = llm)"
      ],
      "metadata": {
        "id": "wXaEJcbLBw4X"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent = initialize_agent(tools, llm, agent = 'zero-shot-react-description', verbose = True)"
      ],
      "metadata": {
        "id": "Co-bSoyECGhg"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent.run('who is the current leader of Japan? What is the largest prime that is smaller than their age?')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "wmkeEORwC2vp",
        "outputId": "8c944d33-e3f7-47be-9cd5-c328c2755835"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m I need to find out the age of the leader of Japan first.\n",
            "Action: Search\n",
            "Action Input: who is the current leader of Japan\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3mFumio Kishida\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m I should find out the age of Fumio Kishida \n",
            "Action: Search\n",
            "Action Input: age of Fumio Kishida\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3m65 years\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m I can use the Calculator to find out the largest prime that is smaller than 65\n",
            "Action: Calculator\n",
            "Action Input: largest prime smaller than 65\u001b[0m\n",
            "Observation: \u001b[33;1m\u001b[1;3mAnswer: 61\n",
            "\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer\n",
            "Final Answer: Fumio Kishida is the current leader of Japan, and the largest prime that is smaller than their age is 61.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Fumio Kishida is the current leader of Japan, and the largest prime that is smaller than their age is 61.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Memory: Add State to chains and agents"
      ],
      "metadata": {
        "id": "O35nGx3eDoA_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import OpenAI, ConversationChain"
      ],
      "metadata": {
        "id": "OdQPjEHODgNS"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = OpenAI(temperature = 0.9)\n",
        "conversation = ConversationChain(llm = llm, verbose = True)"
      ],
      "metadata": {
        "id": "gFaa-PGmFdR5"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.predict(input = 'Hi there')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "id": "A4SWVeu2FtYH",
        "outputId": "daec39fe-4f91-4d0f-c129-67aef56b2962"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "\n",
            "Human: Hi there\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\" Hi! How are you doing today? I'm doing great - the weather is lovely and I'm excited for upcoming discussions about AI. What about you?\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.predict(input = 'The weather is terrible today and I am feeling terrible.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "id": "IZJbcbtbFy5w",
        "outputId": "e01c5141-7d30-471f-fe77-b98fb8894902"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: Hi there\n",
            "AI:  Hi! How are you doing today? I'm doing great - the weather is lovely and I'm excited for upcoming discussions about AI. What about you?\n",
            "Human: The weather is terrible today and I am feeling terrible.\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\" Oh, I'm sorry to hear that. Is there anything I can do to help make your day better?\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.predict(input = 'I hope, you can give me advice to feel better.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341
        },
        "id": "u1SvM_o3F6xK",
        "outputId": "9c49ee5a-978b-43cd-a0ef-256fb8dc253e"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: Hi there\n",
            "AI:  Hi! How are you doing today? I'm doing great - the weather is lovely and I'm excited for upcoming discussions about AI. What about you?\n",
            "Human: The weather is terrible today and I am feeling terrible.\n",
            "AI:  Oh, I'm sorry to hear that. Is there anything I can do to help make your day better?\n",
            "Human: I hope, you can give me advice to feel better.\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\" Absolutely! Are there any particular topics or situations that are bothering you? I'm happy to offer advice or to provide a listening ear if that would be more helpful.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Agents\n"
      ],
      "metadata": {
        "id": "KC0DGd0P0BzA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents import load_tools\n",
        "from langchain.agents import initialize_agent\n",
        "from langchain.llms import OpenAI"
      ],
      "metadata": {
        "id": "9rJ_7PznHKTq"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = OpenAI(temperature = 0)"
      ],
      "metadata": {
        "id": "EfzwoiElHQIq"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tool_names = ['serpapi']\n",
        "tools = load_tools(tool_names)"
      ],
      "metadata": {
        "id": "ulCYoiHRHUWC"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent = initialize_agent(tools, llm, agent = 'zero-shot-react-description', verbose = True)\n"
      ],
      "metadata": {
        "id": "hb41nP_R1B4z"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent.run('What is LangChain')"
      ],
      "metadata": {
        "id": "CjKyP_au1LQS",
        "outputId": "b94b854d-7253-43da-a9d9-6ebc5a06e89c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        }
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m I should research what LangChain is\n",
            "Action: Search\n",
            "Action Input: \"LangChain\"\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3mLangChain provides a standard interface for chains, lots of integrations with other tools, and end-to-end chains for common applications. Indexes: Language ...\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m I now know what LangChain is\n",
            "Final Answer: LangChain is a platform that provides a standard interface for chains, lots of integrations with other tools, and end-to-end chains for common applications. It also provides indexes for language-specific data.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'LangChain is a platform that provides a standard interface for chains, lots of integrations with other tools, and end-to-end chains for common applications. It also provides indexes for language-specific data.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "agent.run('Who is the CEO of Coca-Cola')"
      ],
      "metadata": {
        "id": "iCjdwkmm1OSO",
        "outputId": "b739c4f4-4607-4802-e80d-dcef6d4d36f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        }
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m I need to find out who the CEO of Coca-Cola is\n",
            "Action: Search\n",
            "Action Input: \"Coca-Cola CEO\"\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3mJames Quincey\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer\n",
            "Final Answer: James Quincey is the CEO of Coca-Cola.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'James Quincey is the CEO of Coca-Cola.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ask Question on your custom(or piravite)File + ChatGPT"
      ],
      "metadata": {
        "id": "2PuQN5OJ2D1o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-magic -q\n",
        "!pip install unstructured -q\n",
        "!pip install chromadb -q"
      ],
      "metadata": {
        "id": "aRJhFbLf2-uN",
        "outputId": "ca61e05e-05e4-46d0-9d57-9fde7affce85",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m85.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m101.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m85.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 KB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m238.1/238.1 KB\u001b[0m \u001b[31m30.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.8/77.8 KB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m152.8/152.8 KB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.6/69.6 KB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.1/51.1 KB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 KB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.6/80.6 KB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for unstructured (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for python-docx (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for python-pptx (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.4/41.4 KB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.8/62.8 KB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m925.2/925.2 KB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.1/57.1 KB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.2/15.2 MB\u001b[0m \u001b[31m100.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.8/57.8 KB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 KB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m102.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m85.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.9/66.9 KB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m91.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m81.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.8/199.8 KB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m116.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.5/106.5 KB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m75.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m417.9/417.9 KB\u001b[0m \u001b[31m45.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m102.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for hnswlib (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.vectorstores import Chroma \n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.document_loaders import DirectoryLoader\n",
        "from langchain import VectorDBQA\n",
        "import magic \n",
        "import os \n",
        "import nltk"
      ],
      "metadata": {
        "id": "8JtEJMTg1zJZ"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.listdir('drive/MyDrive/transfer/p_graham_essays')"
      ],
      "metadata": {
        "id": "dbMl4J9t2vpv",
        "outputId": "a2070886-b241-4378-8474-9d3bedc35ad2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['p1.txt', 'p2.txt', 'p3.txt']"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loader = DirectoryLoader('drive/MyDrive/transfer/p_graham_essays/', glob = '**/*.txt')"
      ],
      "metadata": {
        "id": "fWu1O2Jx4ULm"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documents = loader.load()"
      ],
      "metadata": {
        "id": "9jLITCyz5Bps",
        "outputId": "ac385a76-9c2a-4766-ff76-9e884d9d8f73",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "documents[0]"
      ],
      "metadata": {
        "id": "IarN7WjN5G5c",
        "outputId": "96ac16bc-4bbe-46fa-b5f2-d3483bf8ecbf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(page_content='September 2017\\n\\nThe most valuable insights are both general and surprising.\\n\\nF&nbsp;=&nbsp;ma for example. But general and surprising is a hard\\n\\ncombination to achieve. That territory tends to be picked\\n\\nclean, precisely because those insights are so valuable.\\n\\nOrdinarily, the best that people can do is one without the\\n\\nother: either surprising without being general (e.g.\\n\\ngossip), or general without being surprising (e.g.\\n\\nplatitudes).\\n\\nWhere things get interesting is the moderately valuable\\n\\ninsights.  You get those from small additions of whichever\\n\\nquality was missing.  The more common case is a small\\n\\naddition of generality: a piece of gossip that\\'s more than\\n\\njust gossip, because it teaches something interesting about\\n\\nthe world. But another less common approach is to focus on\\n\\nthe most general ideas and see if you can find something new\\n\\nto say about them. Because these start out so general, you\\n\\nonly need a small delta of novelty to produce a useful\\n\\ninsight.\\n\\nA small delta of novelty is all you\\'ll be able to get most\\n\\nof the time. Which means if you take this route your ideas\\n\\nwill seem a lot like ones that already exist. Sometimes\\n\\nyou\\'ll find you\\'ve merely rediscovered an idea that did\\n\\nalready exist.  But don\\'t be discouraged.  Remember the huge\\n\\nmultiplier that kicks in when you do manage to think of\\n\\nsomething even a little new.\\n\\nCorollary: the more general the ideas you\\'re talking about,\\n\\nthe less you should worry about repeating yourself.  If you\\n\\nwrite enough, it\\'s inevitable you will.  Your brain is much\\n\\nthe same from year to year and so are the stimuli that hit\\n\\nit. I feel slightly bad when I find I\\'ve said something\\n\\nclose to what I\\'ve said before, as if I were plagiarizing\\n\\nmyself. But rationally one shouldn\\'t.  You won\\'t say\\n\\nsomething exactly the same way the second time, and that\\n\\nvariation increases the chance you\\'ll get that tiny but\\n\\ncritical delta of novelty.\\n\\nAnd of course, ideas beget ideas.  (That sounds familiar.)\\n\\nAn idea with a small amount of novelty could lead to one\\n\\nwith more. But only if you keep going. So it\\'s doubly\\n\\nimportant not to let yourself be discouraged by people who\\n\\nsay there\\'s not much new about something you\\'ve discovered.\\n\\n\"Not much new\" is a real achievement when you\\'re talking\\n\\nabout the most general ideas. Maybe if you keep going,\\n\\nyou\\'ll discover more.\\n\\nIt\\'s not true that there\\'s nothing new under the sun.  There\\n\\nare some domains where there\\'s almost nothing new.  But\\n\\nthere\\'s a big difference between nothing and almost nothing,\\n\\nwhen it\\'s multiplied by the area under the sun.\\n\\n<b>Thanks</b> to Sam Altman, Patrick Collison, and Jessica\\n\\nLivingston for reading drafts of this.', lookup_str='', metadata={'source': 'drive/MyDrive/transfer/p_graham_essays/p1.txt'}, lookup_index=0)"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_splitter = CharacterTextSplitter(chunk_size = 1000, chunk_overlap = 0)"
      ],
      "metadata": {
        "id": "UTsDS_sa6MAE"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts = text_splitter.split_documents(documents)"
      ],
      "metadata": {
        "id": "pr4jYioR6i0u"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts"
      ],
      "metadata": {
        "id": "kCif79q66n-b",
        "outputId": "2f36b427-1919-49ce-df05-84efa6c998d9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content=\"September 2017\\n\\nThe most valuable insights are both general and surprising.\\n\\nF&nbsp;=&nbsp;ma for example. But general and surprising is a hard\\n\\ncombination to achieve. That territory tends to be picked\\n\\nclean, precisely because those insights are so valuable.\\n\\nOrdinarily, the best that people can do is one without the\\n\\nother: either surprising without being general (e.g.\\n\\ngossip), or general without being surprising (e.g.\\n\\nplatitudes).\\n\\nWhere things get interesting is the moderately valuable\\n\\ninsights.  You get those from small additions of whichever\\n\\nquality was missing.  The more common case is a small\\n\\naddition of generality: a piece of gossip that's more than\\n\\njust gossip, because it teaches something interesting about\\n\\nthe world. But another less common approach is to focus on\\n\\nthe most general ideas and see if you can find something new\\n\\nto say about them. Because these start out so general, you\\n\\nonly need a small delta of novelty to produce a useful\\n\\ninsight.\", lookup_str='', metadata={'source': 'drive/MyDrive/transfer/p_graham_essays/p1.txt'}, lookup_index=0),\n",
              " Document(page_content=\"A small delta of novelty is all you'll be able to get most\\n\\nof the time. Which means if you take this route your ideas\\n\\nwill seem a lot like ones that already exist. Sometimes\\n\\nyou'll find you've merely rediscovered an idea that did\\n\\nalready exist.  But don't be discouraged.  Remember the huge\\n\\nmultiplier that kicks in when you do manage to think of\\n\\nsomething even a little new.\\n\\nCorollary: the more general the ideas you're talking about,\\n\\nthe less you should worry about repeating yourself.  If you\\n\\nwrite enough, it's inevitable you will.  Your brain is much\\n\\nthe same from year to year and so are the stimuli that hit\\n\\nit. I feel slightly bad when I find I've said something\\n\\nclose to what I've said before, as if I were plagiarizing\\n\\nmyself. But rationally one shouldn't.  You won't say\\n\\nsomething exactly the same way the second time, and that\\n\\nvariation increases the chance you'll get that tiny but\\n\\ncritical delta of novelty.\\n\\nAnd of course, ideas beget ideas.  (That sounds familiar.)\", lookup_str='', metadata={'source': 'drive/MyDrive/transfer/p_graham_essays/p1.txt'}, lookup_index=0),\n",
              " Document(page_content='An idea with a small amount of novelty could lead to one\\n\\nwith more. But only if you keep going. So it\\'s doubly\\n\\nimportant not to let yourself be discouraged by people who\\n\\nsay there\\'s not much new about something you\\'ve discovered.\\n\\n\"Not much new\" is a real achievement when you\\'re talking\\n\\nabout the most general ideas. Maybe if you keep going,\\n\\nyou\\'ll discover more.\\n\\nIt\\'s not true that there\\'s nothing new under the sun.  There\\n\\nare some domains where there\\'s almost nothing new.  But\\n\\nthere\\'s a big difference between nothing and almost nothing,\\n\\nwhen it\\'s multiplied by the area under the sun.\\n\\n<b>Thanks</b> to Sam Altman, Patrick Collison, and Jessica\\n\\nLivingston for reading drafts of this.', lookup_str='', metadata={'source': 'drive/MyDrive/transfer/p_graham_essays/p1.txt'}, lookup_index=0),\n",
              " Document(page_content=\"January 2017\\n\\nPeople who are powerful but uncharismatic will tend to be disliked.\\nTheir power makes them a target for criticism that they don't have\\nthe charisma to disarm. That was Hillary Clinton's problem. It also\\ntends to be a problem for any CEO who is more of a builder than a\\nschmoozer. And yet the builder-type CEO is (like Hillary) probably\\nthe best person for the job.\\n\\nI don't think there is any solution to this problem. It's human\\nnature. The best we can do is to recognize that it's happening, and\\nto understand that being a magnet for criticism is sometimes a sign\\nnot that someone is the wrong person for a job, but that they're\\nthe right one.\\nJanuary 2017\\n\\nBecause biographies of famous scientists tend to \\nedit out their mistakes, we underestimate the \\ndegree of risk they were willing to take.\\nAnd because anything a famous scientist did that\\nwasn't a mistake has probably now become the\\nconventional wisdom, those choices don't\\nseem risky either.\", lookup_str='', metadata={'source': 'drive/MyDrive/transfer/p_graham_essays/p2.txt'}, lookup_index=0),\n",
              " Document(page_content='Biographies of Newton, for example, understandably focus\\nmore on physics than alchemy or theology.\\nThe impression we get is that his unerring judgment\\nled him straight to truths no one else had noticed.\\nHow to explain all the time he spent on alchemy\\nand theology?  Well, smart people are often kind of\\ncrazy.\\n\\nBut maybe there is a simpler explanation. Maybe\\nthe smartness and the craziness were not as separate\\nas we think. Physics seems to us a promising thing\\nto work on, and alchemy and theology obvious wastes\\nof time. But that\\'s because we know how things\\nturned out. In Newton\\'s day the three problems \\nseemed roughly equally promising. No one knew yet\\nwhat the payoff would be for inventing what we\\nnow call physics; if they had, more people would \\nhave been working on it. And alchemy and theology\\nwere still then in the category Marc Andreessen would \\ndescribe as \"huge, if true.\"\\n\\nNewton made three bets. One of them worked. But \\nthey were all risky.\\nNovember 2016', lookup_str='', metadata={'source': 'drive/MyDrive/transfer/p_graham_essays/p2.txt'}, lookup_index=0),\n",
              " Document(page_content=\"If you're a California voter, there is an important proposition\\non your ballot this year: Proposition 62, which bans the death\\npenalty.\\n\\nWhen I was younger I used to think the debate about the death\\npenalty was about when it's ok to take a human life.  Is it ok\\nto kill a killer?\\n\\nBut that is not the issue here.\\n\\nThe real world does not work like the version I was shown on TV growing up.  The police \\noften arrest the wrong person.\\nDefendants' lawyers are often incompetent.  And prosecutors\\nare often motivated more by publicity than justice.\\n\\nIn the real world,       \\nabout 4% of people sentenced to death\\nare innocent.\\nSo this is not about whether it's ok to kill killers. This\\nis about whether it's ok to kill innocent people.\\n\\nA child could answer that one for you.\", lookup_str='', metadata={'source': 'drive/MyDrive/transfer/p_graham_essays/p2.txt'}, lookup_index=0),\n",
              " Document(page_content=\"This year, in California, you have a chance to end this, by\\nvoting yes on Proposition 62. But beware, because there is another \\nproposition, Proposition 66, whose goal is to make it \\neasier to execute people. So yes on 62, no on 66.\\n\\nIt's time.\", lookup_str='', metadata={'source': 'drive/MyDrive/transfer/p_graham_essays/p2.txt'}, lookup_index=0),\n",
              " Document(page_content=\"ramen\\nprofitability before Demo Day.  Though it sounds slightly\\nparadoxical, if you want to raise money, the best thing you can do\\nis get yourself to the point where you don't need to.\\n\\nThere are almost two distinct modes of fundraising: one in which\\nfounders who need money knock on doors seeking it, knowing that\\notherwise the company will die or at the very least people will\\nhave to be fired, and one in which founders who don't need money\\ntake some to grow faster than they could merely on their own revenues.\\nTo emphasize the distinction I'm going to name them: type A fundraising\\nis when you don't need money, and type B fundraising is when you\\ndo.\", lookup_str='', metadata={'source': 'drive/MyDrive/transfer/p_graham_essays/p3.txt'}, lookup_index=0),\n",
              " Document(page_content=\"Inexperienced founders read about famous startups doing what was\\ntype A fundraising, and decide they should raise money too, since\\nthat seems to be how startups work. Except when they raise money\\nthey don't have a clear path to profitability and are thus doing\\ntype B fundraising.  And they are then surprised how difficult and\\nunpleasant it is.\\n\\nOf course not all startups can make it to ramen profitability in a\\nfew months.  And some that don't still manage to have the upper\\nhand over investors, if they have some other advantage like\\nextraordinary growth numbers or exceptionally formidable founders.\\nBut as time passes it gets increasingly difficult to fundraise from\\na position of strength without being profitable.\\n\\n[19]\", lookup_str='', metadata={'source': 'drive/MyDrive/transfer/p_graham_essays/p3.txt'}, lookup_index=0)]"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = OpenAIEmbeddings(openai_api_key= os.environ['OPENAI_API_KEY'])"
      ],
      "metadata": {
        "id": "xdIXacYN6pv8"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docsearch = Chroma.from_documents(texts, embeddings)"
      ],
      "metadata": {
        "id": "xFE9o4il7Ocd",
        "outputId": "fef261d0-cc32-4faa-e4d5-bfe1a8f202a1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG:Chroma:Logger created\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Chroma using direct local API.\n",
            "Using DuckDB in-memory for database. Data will be transient.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG:Chroma:Index not found\n",
            "DEBUG:Chroma:Index saved to .chroma/index/index.bin\n",
            "DEBUG:Chroma:Index saved to .chroma/index/index.bin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "qa = VectorDBQA.from_chain_type(llm = OpenAI(), chain_type = 'stuff', vectorstore = docsearch)"
      ],
      "metadata": {
        "id": "Iq6xjibN9Hks"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = 'A small delta of novelty is all you\\'ll be able to get most\\n\\nof the time. Which means if you take this route your ideas\\n\\nwill seem a lot like ones that already exist. Sometimes\\n\\nyou\\'ll find you\\'ve merely rediscovered an idea that did\\n\\nalready exist.  But don\\'t be discouraged.  Remember the huge\\n\\nmultiplier that kicks in when you do manage to think of\\n\\nsomething even a little new.\\n\\nCorollary: the more general the ideas you\\'re talking about,\\n\\nthe less you should worry about repeating yourself.?'\n",
        "qa.run(query)"
      ],
      "metadata": {
        "id": "5oB5UehY-Jsj",
        "outputId": "3a031eab-2d86-411f-b850-7c51aa703f4a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG:Chroma:time to pre process our knn query: 1.1920928955078125e-06\n",
            "DEBUG:Chroma:time to run knn query: 0.00012111663818359375\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\" When discussing general ideas, it is okay to repeat yourself. Even if you are repeating an idea that already exists, there is still potential for a small delta of novelty that can lead to more ideas. Don't be discouraged if you find yourself discussing ideas that have already been explored; keep going and you may be able to add something new.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "qa = VectorDBQA.from_chain_type(llm=OpenAI(), chain_type=\"stuff\", vectorstore=docsearch, return_source_documents=True)\n",
        "query = \"What were there in the Biographies of Newton ?\"\n",
        "result = qa({\"query\": query})"
      ],
      "metadata": {
        "id": "dDiwjjiH_euR",
        "outputId": "63b0896a-ac54-449d-c04d-af240c804a45",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG:Chroma:time to pre process our knn query: 1.1920928955078125e-06\n",
            "DEBUG:Chroma:time to run knn query: 0.0001239776611328125\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result"
      ],
      "metadata": {
        "id": "dvOl6MpnAxjm",
        "outputId": "f8285ce2-d0e4-4bfb-fb27-eb1f39809efd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'query': 'What were there in the Biographies of Newton ?',\n",
              " 'result': \" They focused more on Newton's physics than on his alchemy or theology.\",\n",
              " 'source_documents': [Document(page_content='Biographies of Newton, for example, understandably focus\\nmore on physics than alchemy or theology.\\nThe impression we get is that his unerring judgment\\nled him straight to truths no one else had noticed.\\nHow to explain all the time he spent on alchemy\\nand theology?  Well, smart people are often kind of\\ncrazy.\\n\\nBut maybe there is a simpler explanation. Maybe\\nthe smartness and the craziness were not as separate\\nas we think. Physics seems to us a promising thing\\nto work on, and alchemy and theology obvious wastes\\nof time. But that\\'s because we know how things\\nturned out. In Newton\\'s day the three problems \\nseemed roughly equally promising. No one knew yet\\nwhat the payoff would be for inventing what we\\nnow call physics; if they had, more people would \\nhave been working on it. And alchemy and theology\\nwere still then in the category Marc Andreessen would \\ndescribe as \"huge, if true.\"\\n\\nNewton made three bets. One of them worked. But \\nthey were all risky.\\nNovember 2016', lookup_str='', metadata={'source': 'drive/MyDrive/transfer/p_graham_essays/p2.txt'}, lookup_index=0),\n",
              "  Document(page_content=\"January 2017\\n\\nPeople who are powerful but uncharismatic will tend to be disliked.\\nTheir power makes them a target for criticism that they don't have\\nthe charisma to disarm. That was Hillary Clinton's problem. It also\\ntends to be a problem for any CEO who is more of a builder than a\\nschmoozer. And yet the builder-type CEO is (like Hillary) probably\\nthe best person for the job.\\n\\nI don't think there is any solution to this problem. It's human\\nnature. The best we can do is to recognize that it's happening, and\\nto understand that being a magnet for criticism is sometimes a sign\\nnot that someone is the wrong person for a job, but that they're\\nthe right one.\\nJanuary 2017\\n\\nBecause biographies of famous scientists tend to \\nedit out their mistakes, we underestimate the \\ndegree of risk they were willing to take.\\nAnd because anything a famous scientist did that\\nwasn't a mistake has probably now become the\\nconventional wisdom, those choices don't\\nseem risky either.\", lookup_str='', metadata={'source': 'drive/MyDrive/transfer/p_graham_essays/p2.txt'}, lookup_index=0),\n",
              "  Document(page_content='An idea with a small amount of novelty could lead to one\\n\\nwith more. But only if you keep going. So it\\'s doubly\\n\\nimportant not to let yourself be discouraged by people who\\n\\nsay there\\'s not much new about something you\\'ve discovered.\\n\\n\"Not much new\" is a real achievement when you\\'re talking\\n\\nabout the most general ideas. Maybe if you keep going,\\n\\nyou\\'ll discover more.\\n\\nIt\\'s not true that there\\'s nothing new under the sun.  There\\n\\nare some domains where there\\'s almost nothing new.  But\\n\\nthere\\'s a big difference between nothing and almost nothing,\\n\\nwhen it\\'s multiplied by the area under the sun.\\n\\n<b>Thanks</b> to Sam Altman, Patrick Collison, and Jessica\\n\\nLivingston for reading drafts of this.', lookup_str='', metadata={'source': 'drive/MyDrive/transfer/p_graham_essays/p1.txt'}, lookup_index=0),\n",
              "  Document(page_content=\"A small delta of novelty is all you'll be able to get most\\n\\nof the time. Which means if you take this route your ideas\\n\\nwill seem a lot like ones that already exist. Sometimes\\n\\nyou'll find you've merely rediscovered an idea that did\\n\\nalready exist.  But don't be discouraged.  Remember the huge\\n\\nmultiplier that kicks in when you do manage to think of\\n\\nsomething even a little new.\\n\\nCorollary: the more general the ideas you're talking about,\\n\\nthe less you should worry about repeating yourself.  If you\\n\\nwrite enough, it's inevitable you will.  Your brain is much\\n\\nthe same from year to year and so are the stimuli that hit\\n\\nit. I feel slightly bad when I find I've said something\\n\\nclose to what I've said before, as if I were plagiarizing\\n\\nmyself. But rationally one shouldn't.  You won't say\\n\\nsomething exactly the same way the second time, and that\\n\\nvariation increases the chance you'll get that tiny but\\n\\ncritical delta of novelty.\\n\\nAnd of course, ideas beget ideas.  (That sounds familiar.)\", lookup_str='', metadata={'source': 'drive/MyDrive/transfer/p_graham_essays/p1.txt'}, lookup_index=0)]}"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result['source_documents']"
      ],
      "metadata": {
        "id": "6hMYo9RYAdcO",
        "outputId": "d3dfe557-b03c-4f80-ce5a-c7549b40ba0e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content='Biographies of Newton, for example, understandably focus\\nmore on physics than alchemy or theology.\\nThe impression we get is that his unerring judgment\\nled him straight to truths no one else had noticed.\\nHow to explain all the time he spent on alchemy\\nand theology?  Well, smart people are often kind of\\ncrazy.\\n\\nBut maybe there is a simpler explanation. Maybe\\nthe smartness and the craziness were not as separate\\nas we think. Physics seems to us a promising thing\\nto work on, and alchemy and theology obvious wastes\\nof time. But that\\'s because we know how things\\nturned out. In Newton\\'s day the three problems \\nseemed roughly equally promising. No one knew yet\\nwhat the payoff would be for inventing what we\\nnow call physics; if they had, more people would \\nhave been working on it. And alchemy and theology\\nwere still then in the category Marc Andreessen would \\ndescribe as \"huge, if true.\"\\n\\nNewton made three bets. One of them worked. But \\nthey were all risky.\\nNovember 2016', lookup_str='', metadata={'source': 'drive/MyDrive/transfer/p_graham_essays/p2.txt'}, lookup_index=0),\n",
              " Document(page_content=\"January 2017\\n\\nPeople who are powerful but uncharismatic will tend to be disliked.\\nTheir power makes them a target for criticism that they don't have\\nthe charisma to disarm. That was Hillary Clinton's problem. It also\\ntends to be a problem for any CEO who is more of a builder than a\\nschmoozer. And yet the builder-type CEO is (like Hillary) probably\\nthe best person for the job.\\n\\nI don't think there is any solution to this problem. It's human\\nnature. The best we can do is to recognize that it's happening, and\\nto understand that being a magnet for criticism is sometimes a sign\\nnot that someone is the wrong person for a job, but that they're\\nthe right one.\\nJanuary 2017\\n\\nBecause biographies of famous scientists tend to \\nedit out their mistakes, we underestimate the \\ndegree of risk they were willing to take.\\nAnd because anything a famous scientist did that\\nwasn't a mistake has probably now become the\\nconventional wisdom, those choices don't\\nseem risky either.\", lookup_str='', metadata={'source': 'drive/MyDrive/transfer/p_graham_essays/p2.txt'}, lookup_index=0),\n",
              " Document(page_content='An idea with a small amount of novelty could lead to one\\n\\nwith more. But only if you keep going. So it\\'s doubly\\n\\nimportant not to let yourself be discouraged by people who\\n\\nsay there\\'s not much new about something you\\'ve discovered.\\n\\n\"Not much new\" is a real achievement when you\\'re talking\\n\\nabout the most general ideas. Maybe if you keep going,\\n\\nyou\\'ll discover more.\\n\\nIt\\'s not true that there\\'s nothing new under the sun.  There\\n\\nare some domains where there\\'s almost nothing new.  But\\n\\nthere\\'s a big difference between nothing and almost nothing,\\n\\nwhen it\\'s multiplied by the area under the sun.\\n\\n<b>Thanks</b> to Sam Altman, Patrick Collison, and Jessica\\n\\nLivingston for reading drafts of this.', lookup_str='', metadata={'source': 'drive/MyDrive/transfer/p_graham_essays/p1.txt'}, lookup_index=0),\n",
              " Document(page_content=\"A small delta of novelty is all you'll be able to get most\\n\\nof the time. Which means if you take this route your ideas\\n\\nwill seem a lot like ones that already exist. Sometimes\\n\\nyou'll find you've merely rediscovered an idea that did\\n\\nalready exist.  But don't be discouraged.  Remember the huge\\n\\nmultiplier that kicks in when you do manage to think of\\n\\nsomething even a little new.\\n\\nCorollary: the more general the ideas you're talking about,\\n\\nthe less you should worry about repeating yourself.  If you\\n\\nwrite enough, it's inevitable you will.  Your brain is much\\n\\nthe same from year to year and so are the stimuli that hit\\n\\nit. I feel slightly bad when I find I've said something\\n\\nclose to what I've said before, as if I were plagiarizing\\n\\nmyself. But rationally one shouldn't.  You won't say\\n\\nsomething exactly the same way the second time, and that\\n\\nvariation increases the chance you'll get that tiny but\\n\\ncritical delta of novelty.\\n\\nAnd of course, ideas beget ideas.  (That sounds familiar.)\", lookup_str='', metadata={'source': 'drive/MyDrive/transfer/p_graham_essays/p1.txt'}, lookup_index=0)]"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exempla Selector"
      ],
      "metadata": {
        "id": "UPFG-teM_47Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import FewShotPromptTemplate"
      ],
      "metadata": {
        "id": "ueHIh7E9Al9B"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.prompts.example_selector import LengthBasedExampleSelector"
      ],
      "metadata": {
        "id": "EeOdAinFAD9Z"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# These are a lot of examples of a pretend task of creating antonyms.\n",
        "examples = [\n",
        "    {\"input\": \"happy\", \"output\": \"sad\"},\n",
        "    {\"input\": \"tall\", \"output\": \"short\"},\n",
        "    {\"input\": \"energetic\", \"output\": \"lethargic\"},\n",
        "    {\"input\": \"sunny\", \"output\": \"gloomy\"},\n",
        "    {\"input\": \"windy\", \"output\": \"calm\"},\n",
        "]"
      ],
      "metadata": {
        "id": "dXvFplgSBqQW"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_prompt = PromptTemplate(\n",
        "    input_variables = ['input','output'],\n",
        "    template = 'Input: {input}\\nOutput: {output}'\n",
        "    )\n",
        "\n",
        "example_selector = LengthBasedExampleSelector(\n",
        "    examples = examples,\n",
        "    example_prompt = example_prompt,\n",
        "    max_lenth = 25\n",
        "    )\n",
        "# import re\n",
        "dynamic_prompt = FewShotPromptTemplate(\n",
        "    example_selector = example_selector,\n",
        "    example_prompt = example_prompt,\n",
        "    prefix = 'Give the antonym of every input',\n",
        "    suffix = 'Input: {adjective}\\nOutput',\n",
        "    input_variables = ['adjective'],\n",
        "    # max_length = 25\n",
        "    # get_text_length= lambda x: len(re.split(\"\\n| \", x))\n",
        ")\n"
      ],
      "metadata": {
        "id": "A69i38x7Bsav"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(dynamic_prompt.format(adjective = 'big'))"
      ],
      "metadata": {
        "id": "EY7UJ0eIDRC0",
        "outputId": "ff9231c4-282e-4e84-ee8d-e27b3f458d02",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Give the antonym of every input\n",
            "\n",
            "Input: happy\n",
            "Output: sad\n",
            "\n",
            "Input: tall\n",
            "Output: short\n",
            "\n",
            "Input: energetic\n",
            "Output: lethargic\n",
            "\n",
            "Input: sunny\n",
            "Output: gloomy\n",
            "\n",
            "Input: windy\n",
            "Output: calm\n",
            "\n",
            "Input: big\n",
            "Output\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# An example with long input, so it selects only one example.\n",
        "long_string = \"big and huge and massive and large and gigantic and tall and much much much much much bigger than everything else\"\n",
        "print(dynamic_prompt.format(adjective=long_string))"
      ],
      "metadata": {
        "id": "jdeUqTq7HiJW",
        "outputId": "d3e56ecf-466b-4f0e-b9af-9981b310fb76",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Give the antonym of every input\n",
            "\n",
            "Input: happy\n",
            "Output: sad\n",
            "\n",
            "Input: tall\n",
            "Output: short\n",
            "\n",
            "Input: energetic\n",
            "Output: lethargic\n",
            "\n",
            "Input: sunny\n",
            "Output: gloomy\n",
            "\n",
            "Input: windy\n",
            "Output: calm\n",
            "\n",
            "Input: big and huge and massive and large and gigantic and tall and much much much much much bigger than everything else\n",
            "Output\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dynamic_prompt"
      ],
      "metadata": {
        "id": "F1uVt-e8HpXX",
        "outputId": "ee5a9ed7-b721-4f81-c951-4ce4d8984788",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FewShotPromptTemplate(input_variables=['adjective'], output_parser=None, partial_variables={}, examples=None, example_selector=LengthBasedExampleSelector(examples=[{'input': 'happy', 'output': 'sad'}, {'input': 'tall', 'output': 'short'}, {'input': 'energetic', 'output': 'lethargic'}, {'input': 'sunny', 'output': 'gloomy'}, {'input': 'windy', 'output': 'calm'}], example_prompt=PromptTemplate(input_variables=['input', 'output'], output_parser=None, partial_variables={}, template='Input: {input}\\nOutput: {output}', template_format='f-string', validate_template=True), get_text_length=<function _get_length_based at 0x7fae798e2940>, max_length=2048, example_text_lengths=[4, 4, 4, 4, 4]), example_prompt=PromptTemplate(input_variables=['input', 'output'], output_parser=None, partial_variables={}, template='Input: {input}\\nOutput: {output}', template_format='f-string', validate_template=True), suffix='Input: {adjective}\\nOutput', example_separator='\\n\\n', prefix='Give the antonym of every input', template_format='f-string', validate_template=True)"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Similarity Example Selector"
      ],
      "metadata": {
        "id": "nXddLsoXD0f-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts.example_selector import SemanticSimilarityExampleSelector\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.embeddings import OpenAIEmbeddings"
      ],
      "metadata": {
        "id": "VohOl5zPDcVz"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_selector = SemanticSimilarityExampleSelector.from_examples(\n",
        "    examples, \n",
        "    OpenAIEmbeddings(),\n",
        "    Chroma,\n",
        "    k=1\n",
        ")\n",
        "\n",
        "similar_prompt = FewShotPromptTemplate(\n",
        "    example_selector = example_selector,\n",
        "    example_prompt = example_prompt,\n",
        "    prefix = 'Give the antonym of every input',\n",
        "    suffix=\"Input: {adjective}\\nOutput:\", \n",
        "    input_variables=[\"adjective\"],\n",
        ")"
      ],
      "metadata": {
        "id": "h5XWOpPtEQsJ",
        "outputId": "a042899e-37f7-4979-c109-57a5a5b1d8a3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Chroma using direct local API.\n",
            "Using DuckDB in-memory for database. Data will be transient.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG:Chroma:Index not found\n",
            "DEBUG:Chroma:Index saved to .chroma/index/index.bin\n",
            "DEBUG:Chroma:Index saved to .chroma/index/index.bin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Exiting: Cleaning up .chroma directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(similar_prompt.format(adjective= 'worried'))"
      ],
      "metadata": {
        "id": "oDOF0d6pFaIe",
        "outputId": "30c54dee-b8a4-45ab-bf13-60e3625fcb73",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG:Chroma:time to pre process our knn query: 1.430511474609375e-06\n",
            "DEBUG:Chroma:time to run knn query: 0.000141143798828125\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Give the antonym of every input\n",
            "\n",
            "Input: happy\n",
            "Output: sad\n",
            "\n",
            "Input: worried\n",
            "Output:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bWA1zHU-F7KF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}